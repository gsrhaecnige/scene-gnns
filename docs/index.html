<style>
  body {
    margin: 0 auto;
  }
</style>
<html>
  <head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type" />
    <style type="text/css">
      ol {
        margin: 0;
        padding: 0;
      }
      table td,
      table th {
        padding: 0;
      }
      .c11 {
        padding-top: 16pt;
        padding-bottom: 4pt;
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      .c6 {
        padding-top: 18pt;
        padding-bottom: 6pt;
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      .c17 {
        padding-top: 14pt;
        padding-bottom: 4pt;
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      .c20 {
        padding-top: 20pt;
        padding-bottom: 6pt;
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      .c2 {
        padding-top: 0pt;
        padding-bottom: 0pt;
        line-height: 1.15;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      .c12 {
        -webkit-text-decoration-skip: none;
        color: #000000;
        text-decoration: line-through;
        vertical-align: baseline;
        text-decoration-skip-ink: none;
        font-size: 11pt;
      }
      .c18 {
        color: #000000;
        text-decoration: none;
        vertical-align: baseline;
        font-size: 20pt;
        font-style: normal;
      }
      .c1 {
        margin-left: 18pt;
        padding-top: 3pt;
        padding-bottom: 0pt;
        line-height: 1;
        text-align: left;
      }
      .c13 {
        color: #000000;
        text-decoration: none;
        vertical-align: baseline;
        font-size: 16pt;
        font-style: normal;
      }
      .c14 {
        color: #434343;
        text-decoration: none;
        vertical-align: baseline;
        font-size: 14pt;
        font-style: normal;
      }
      .c16 {
        color: #666666;
        text-decoration: none;
        vertical-align: baseline;
        font-size: 12pt;
        font-style: normal;
      }
      .c4 {
        color: #000000;
        text-decoration: none;
        vertical-align: baseline;
        font-size: 11pt;
        font-style: normal;
      }
      .c7 {
        text-decoration-skip-ink: none;
        -webkit-text-decoration-skip: none;
        color: #1155cc;
        text-decoration: underline;
      }
      .c21 {
        background-color: #ffffff;
        max-width: 468pt;
        padding: 72pt 72pt 72pt 72pt;
      }
      .c10 {
        vertical-align: baseline;
        font-size: 11pt;
        font-style: normal;
      }
      .c15 {
        font-weight: 400;
        font-family: "Arial";
      }
      .c3 {
        font-weight: 400;
        font-family: "Times New Roman";
      }
      .c0 {
        font-weight: 700;
        font-family: "Times New Roman";
      }
      .c8 {
        color: inherit;
        text-decoration: inherit;
      }
      .c9 {
        font-style: italic;
      }
      .c19 {
        vertical-align: sub;
      }
      .c5 {
        height: 11pt;
      }
      .title {
        padding-top: 0pt;
        color: #000000;
        font-size: 26pt;
        padding-bottom: 3pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      .subtitle {
        padding-top: 0pt;
        color: #666666;
        font-size: 15pt;
        padding-bottom: 16pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      li {
        color: #000000;
        font-size: 11pt;
        font-family: "Arial";
      }
      p {
        margin: 0;
        color: #000000;
        font-size: 11pt;
        font-family: "Arial";
      }
      h1 {
        padding-top: 20pt;
        color: #000000;
        font-size: 20pt;
        padding-bottom: 6pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      h2 {
        padding-top: 18pt;
        color: #000000;
        font-size: 16pt;
        padding-bottom: 6pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      h3 {
        padding-top: 16pt;
        color: #434343;
        font-size: 14pt;
        padding-bottom: 4pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      h4 {
        padding-top: 14pt;
        color: #666666;
        font-size: 12pt;
        padding-bottom: 4pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      h5 {
        padding-top: 12pt;
        color: #666666;
        font-size: 11pt;
        padding-bottom: 4pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      h6 {
        padding-top: 12pt;
        color: #666666;
        font-size: 11pt;
        padding-bottom: 4pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        font-style: italic;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
    </style>
  </head>
  <body class="c21 doc-content">
    <h1 class="c20" id="h.jeqv0n3a6puh">
      <span class="c3 c18">How Well Can GNNs Model the World?</span>
    </h1>
    <h3 class="c11" id="h.r1sidb3rw2l3">
      <span class="c14 c3"
        >Christy
        Li&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        Gracie
        Sheng&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Claire
        Wang</span
      >
    </h3>
    <p class="c2">
      <span class="c7 c3"
        ><a class="c8" href="mailto:ckl@mit.edu">ckl@mit.edu</a></span
      ><span class="c3"
        >&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span
      ><span class="c7 c3"
        ><a class="c8" href="mailto:grac@mit.edu">grac@mit.edu</a></span
      ><span class="c3"
        >&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span
      ><span class="c3 c7"
        ><a class="c8" href="mailto:clairely@mit.edu">clairely@mit.edu</a></span
      ><span class="c4 c3"
        >&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c1">
      <span class="c7 c3"
        ><a class="c8" href="#h.jfcrpfhzao2o">Introduction</a></span
      >
    </p>
    <p class="c1">
      <span class="c7 c3"
        ><a class="c8" href="#h.vuujay5q361i">Project Motivation</a></span
      >
    </p>
    <p class="c1">
      <span class="c7 c3"
        ><a class="c8" href="#h.go8r2ywqy7yu"
          >Background and Related Work</a
        ></span
      >
    </p>
    <p class="c1">
      <span class="c7 c3"
        ><a class="c8" href="#h.xufol0el8ec2">Methods</a></span
      >
    </p>
    <p class="c1">
      <span class="c7 c3"
        ><a class="c8" href="#h.yimec256tcdb">Experiments and Results</a></span
      >
    </p>
    <p class="c1">
      <span class="c7 c3"
        ><a class="c8" href="#h.h1enr8agkv13"
          >Discussion and Conclusion</a
        ></span
      >
    </p>
    <p class="c1">
      <span class="c7 c3"
        ><a class="c8" href="#h.89jc6sjrd2pp">Future Work</a></span
      >
    </p>
    <p class="c1">
      <span class="c7 c3"
        ><a class="c8" href="#h.mhk4rj93sjez">Works Cited</a></span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <h2 class="c6" id="h.jfcrpfhzao2o">
      <span class="c3 c13">Introduction</span>
    </h2>
    <p class="c2">
      <span class="c3"
        >Effective representation learning has become increasingly crucial for
        developing intelligent systems that can understand and interact with
        complex tasks and environments. A major challenge lies in extracting
        meaningful and structured representations of the world from
        high-dimensional observations. </span
      ><span class="c3 c9">Structured World Models</span
      ><span class="c4 c3"
        >&nbsp;(SWMs) are a class of models that are used to learn abstract
        state representations from observations in an environment. These
        representations offer substantial advantages over holistic, unstructured
        approaches because they are able to learn object-centric representations
        and dynamics without explicit supervision.</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c3">In our project, we revisited the GNN-based </span
      ><span class="c3 c9">Contrastively-trained SWM</span
      ><span class="c4 c3"
        >&nbsp;(C-SWM) developed by Kipf et al. in [1] to investigate the
        architecture&#39;s scalability, generalization capabilities, and
        limitations in more complex scenarios. &nbsp;Our study presents a
        systematic investigation into the boundaries and failure modes of C-SWMs
        across three key dimensions: object-centric representation learning,
        physical complexity, and transfer learning. We extend the original work
        by testing the architecture on environments with varying numbers of
        objects, exploring its capacity to model increasingly complex n-body
        physics problems, and examining its potential for transfer learning
        between different Atari games.
      </span>
    </p>
    <h2 class="c6" id="h.vuujay5q361i">
      <span class="c13 c3">Project Motivation</span>
    </h2>
    <p class="c2">
      <span class="c4 c3"
        >Object-centric representations have emerged as a key paradigm for
        understanding and modeling structured environments, especially for
        physical systems, where entities and their relationships are naturally
        separable. C-SWMs leverage object-centric representations combined with
        GNNs to model interactions between objects, using contrastive learning
        to capture underlying dynamics without requiring explicit labels [2].
        This approach is particularly advantageous for environments where
        disentangling object-level features is critical for generalization, such
        as in multi-object physical simulations.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span
        style="
          overflow: hidden;
          display: inline-block;
          margin: 0px 0px;
          border: 0px solid #000000;
          transform: rotate(0rad) translateZ(0px);
          -webkit-transform: rotate(0rad) translateZ(0px);
          width: 624px;
          height: 154.67px;
        "
        ><img
          alt=""
          src="images/image7.png"
          style="
            width: 624px;
            height: 154.67px;
            margin-left: 0px;
            margin-top: 0px;
            transform: rotate(0rad) translateZ(0px);
            -webkit-transform: rotate(0rad) translateZ(0px);
          "
          title=""
      /></span>
    </p>
    <p class="c2">
      <span class="c0">Figure 1:</span
      ><span class="c4 c3"
        >&nbsp;The C-SWM architecture consists of a CNN object extractor, an MLP
        object encoder, and a GNN transition model and uses contrastive loss.
        This example shows how the colored blocks in the 3D environment are
        transformed into abstract state representations by the C-SWM. Figure
        retrieved from original paper by Kipf et al. [1].</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c3"
        >However, the reliance on accurate object detection and the absence of
        explicit mechanisms for modeling disentangled latent representations can
        limit the applicability of C-SWMs to complex, noisy, real-world
        scenarios. Addressing these limitations requires analyzing the
        interpretability of abstract state representations and identifying both
        success and failure modes of these models, which we cover in this
        project. Having reviewed related research summarized below, we found
        several endeavors which sought to improve the performance and
        functionality of SWMs. These were mainly &ldquo;black box&rdquo;
        experiments that compared metrics but did not explain the reasoning
        behind the model&rsquo;s behavior. In this project, our goal is to
        analyze </span
      ><span class="c3 c9">why.</span>
    </p>
    <h2 class="c6" id="h.go8r2ywqy7yu">
      <span class="c13 c3">Background and Related Work</span>
    </h2>
    <p class="c2">
      <span class="c4 c3"
        >In this section, we provide context for the post-encoding portion of
        the C-SWM architecture, which is the central focus of our
        experiments.</span
      >
    </p>
    <h3 class="c11" id="h.7vnp71kwwfj7"><span class="c14 c3">GNNs</span></h3>
    <p class="c2">
      <span class="c3 c9">Graph Neural Networks</span
      ><span class="c4 c3"
        >&nbsp;(GNNs) are powerful for processing data with relational
        structure, such as social networks and molecular graphs, and constitute
        the abstract state transition model in C-SWMs. GNNs extend traditional
        neural networks by leveraging message-passing mechanisms to aggregate
        information from a node&rsquo;s neighbors, enabling them to capture both
        local and global relational patterns. The advent of the Graph
        Convolutional Network (GCN) exhibited the potential of GNNs in
        semi-supervised learning tasks on graph-structured data [2]. Subsequent
        advancements, like Graph Attention Networks (GATs) have refined the
        architecture to improve scalability and expressiveness [3][4].
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c3"
        >Despite their strengths, GNNs have notable limitations. One primary
        drawback is their computational inefficiency on large-scale graphs, as
        message-passing schemes often require extensive memory and computation.
        To address this, sampling-based methods like GraphSAGE have been
        proposed to reduce resource overhead [5]. Another issue is the </span
      ><span class="c3">oversmoothing</span><span class="c3 c9">&nbsp;</span
      ><span class="c4 c3"
        >problem, where repeated message passing can cause node representations
        to become indistinguishable, limiting the depth of GNNs. Efforts to
        mitigate this include architectural modifications like residual
        connections and improved aggregation functions [6]. Additionally, GNNs
        are inherently limited by their dependence on graph connectivity, which
        can lead to suboptimal performance in graphs with noisy or incomplete
        structures. These challenges underline the need for ongoing research to
        enhance the scalability, expressiveness, and robustness of GNN
        models.</span
      >
    </p>
    <h3 class="c11" id="h.4brmi9ssfywm">
      <span class="c14 c3">Contrastive Learning</span>
    </h3>
    <p class="c2">
      <span class="c3 c9">Contrastive learning</span
      ><span class="c4 c3"
        >&nbsp;is a self-supervised technique that learns representations by
        contrasting similar and dissimilar samples, thereby extracting invariant
        features from raw observations. The contrastive training objective
      </span>
    </p>
    <p class="c2">
      <span
        style="
          overflow: hidden;
          display: inline-block;
          margin: 0px 0px;
          border: 0px solid #000000;
          transform: rotate(0rad) translateZ(0px);
          -webkit-transform: rotate(0rad) translateZ(0px);
          width: 343.11px;
          height: 66.38px;
        "
        ><img
          alt=""
          src="images/image2.png"
          style="
            width: 343.11px;
            height: 66.38px;
            margin-left: 0px;
            margin-top: 0px;
            transform: rotate(0rad) translateZ(0px);
            -webkit-transform: rotate(0rad) translateZ(0px);
          "
          title=""
      /></span>
    </p>
    <p class="c2">
      <span class="c3 c9">N</span><span class="c4 c3">: Number of pairs.</span>
    </p>
    <p class="c2">
      <span class="c3 c9">y</span><span class="c3 c9 c19">i</span
      ><span class="c3 c19">&nbsp;</span
      ><span class="c4 c3">: Binary label for similarity</span>
    </p>
    <p class="c2">
      <span class="c3 c9">D</span
      ><span class="c4 c3">: Distance between embeddings</span>
    </p>
    <p class="c2">
      <span class="c3 c9">m</span><span class="c4 c3">: Margin</span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >ensures that positive examples &mdash;actual transitions from the
        environment&mdash; are closer to one another in representation space,
        while negative examples &mdash;randomly paired states&mdash; are
        distanced apart. This loss formulation allows the C-SWM to learn an
        implicit structure of the environment and focus on meaningful object
        state transitions. The margin-based penalty helps the model to handle
        noise.</span
      >
    </p>
    <h3 class="c11" id="h.g7s1g02ekew6">
      <span class="c14 c3">Structured World Models</span>
    </h3>
    <p class="c2">
      <span class="c3 c9">World models</span
      ><span class="c3"
        >&nbsp;aim to learn an explicit representation of environment dynamics
        to improve sample efficiency, and allow agents to imagine or simulate
        potential future states and outcomes. World models were first officially
        introduced in Ha &amp; Schmidhuber 2018 [7]. In this paper, they relied
        on a VAE and training large RNN models to learn scene information and
        make changes to that scene. Typically, world models consist of a
        representation learning module to encode the observations, a dynamics
        model to predict future states, and a policy to select the best actions. </span
      ><span class="c3 c9">Structured world models</span
      ><span class="c4 c3"
        >&nbsp;leverage representation learning and relational dynamics to model
        complex environments by breaking them into entities and their
        interactions. It encodes objects as latent variables and uses mechanisms
        like graph neural networks to capture relationships, enabling better
        generalization to unseen configurations. Kipf et al. demonstrated the
        potential of C-SWMs in environments with clear object boundaries,
        highlighting their ability to infer object-centric representations in a
        self-supervised manner [1]. Furthermore, Collu et al. in 2023 introduced
        Slot Structured World Models (SSWM) to address limitations of previous
        approaches, particularly the inability of feedforward encoders to
        extract object-centric representations or disentangle multiple objects
        with similar appearances [8]. By combining Slot Attention-based
        object-centric encoders with latent graph-based dynamics models, SSWMs
        achieve superior performance in multi-step prediction tasks</span
      >
    </p>
    <h2 class="c6" id="h.xufol0el8ec2"><span class="c13 c3">Methods</span></h2>
    <p class="c2">
      <span class="c3"
        >To evaluate the efficacy of C-SWMs, we reproduced the experiments
        outlined in the implementation by Kipf et al. [1], utilizing their
        publicly available repository (</span
      ><span class="c7 c3"
        ><a
          class="c8"
          href="https://www.google.com/url?q=https://github.com/tkipf/c-swm&amp;sa=D&amp;source=editors&amp;ust=1733895108878001&amp;usg=AOvVaw1VvkYS8zC3wiIsIo_wPw8n"
          >https://github.com/tkipf/c-swm</a
        ></span
      ><span class="c3"
        >). The core experiments focused on assessing the ability of C-SWMs to
        learn object-centric representations and predict relational dynamics
        across various benchmarks. Prior to running new experiments, we first
        replicated the training pipeline, ensuring the alignment of parameters
        and experimental configurations with the original work. The primary
        datasets and environments used in the reproduction and in new
        experiments include multi-object interaction settings: </span
      ><span class="c3 c9">Atari Pong, Space Invaders, 3-Body Problem</span
      ><span class="c4 c3">.</span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c3"
        >To modernize and extend the original implementation, we updated
        dependencies to work with current Python and library versions, resolving
        compatibility issues. Additionally, we introduced custom scripts to
        facilitate visualization of learned embeddings and relational dynamics,
        providing more intuitive insights into model performance. These updates
        are hosted in our forked repository (</span
      ><span class="c7 c3"
        ><a
          class="c8"
          href="https://www.google.com/url?q=https://github.com/ClaireBookworm/scene-gnns&amp;sa=D&amp;source=editors&amp;ust=1733895108878392&amp;usg=AOvVaw0vT7DkMTY3Ry6R_4t--rlc"
          >https://github.com/ClaireBookworm/scene-gnns</a
        ></span
      ><span class="c4 c3"
        >). Our code for new experiments including inference and transfer
        learning are also present in the repository.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >To evaluate model performance, we employed standard metrics such as
        Hits @ 1 and Mean Reciprocal Rank (MRR), which quantify the accuracy of
        the model&#39;s object predictions and rank-based performance in
        relational reasoning tasks, respectively. These metrics provide a
        comprehensive assessment of the model&rsquo;s ability to generalize to
        unseen configurations and dynamics.</span
      >
    </p>
    <h2 class="c6" id="h.yimec256tcdb">
      <span class="c13 c3">Experiments and Results</span>
    </h2>
    <h3 class="c11" id="h.mqqxzjgkb3fz">
      <span class="c3 c14">Latent Representation Analysis</span>
    </h3>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span
        style="
          overflow: hidden;
          display: inline-block;
          margin: 0px 0px;
          border: 0px solid #000000;
          transform: rotate(0rad) translateZ(0px);
          -webkit-transform: rotate(0rad) translateZ(0px);
          width: 624px;
          height: 158.67px;
        "
        ><img
          alt=""
          src="images/image3.png"
          style="
            width: 624px;
            height: 158.67px;
            margin-left: 0px;
            margin-top: 0px;
            transform: rotate(0rad) translateZ(0px);
            -webkit-transform: rotate(0rad) translateZ(0px);
          "
          title=""
      /></span>
    </p>
    <p class="c2">
      <span class="c0">Table 1</span
      ><span class="c4 c3"
        >: Baselines of our ground state models in comparison to the original
        Kipf et al. 2020 paper results [1]. &ldquo;New&rdquo; represents our
        results and &ldquo;Orig.&rdquo; represents the paper&rsquo;s results.
      </span>
    </p>
    <h4 class="c17" id="h.wcuvoydqu0f">
      <span class="c3 c16">Pong (K = 3, K = 5)</span>
    </h4>
    <p class="c2">
      <span class="c4 c3"
        >The first game experiment we reproduced was Atari Pong, which is a 2D
        interactive multi-object game. Like in the original paper, we used a
        dataset consisting of 50x50x6 tensor observations for training and
        evaluation, and trained for 200 epochs. Pong includes three objects: a
        ball and two blocks. We replicated experiments for K=3 and K=5 where K
        refers to the number of object slots the model will allocate. In the
        paper by Kipf et al., latent representations were not shown for complex
        game environments [1]. Hence, we visualized Pong in our experiments. See
        Figure 2 for the visualization of masks, embeddings, and state
        transitions for the Pong, K=5 experiment.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >A key finding in this experiment was that the encoder was able to
        capture all three game objects according to the object masks.
        Unfortunately, the model appeared to struggle with distinguishing
        between certain objects &mdash;see objects 3 and 5&mdash; and therefore
        treated two objects (in this case the ball and a block) as a single
        object. Nevertheless, it confidently discarded unnecessary object slots
        &mdash;see objects 1, 2, 4. The distance between similar and dissimilar
        objects in embedding space also aligns with the input data, which
        further indicates that the objects are being distinguished. This same
        observation is reflected in the per-object abstract state transition
        graphs. Since the state and action spaces of Pong are expansive, the
        interpretability of the state transition graphs appears to suffer,
        though the plots do imply smooth and continuous transitions over time,
        which remains in accordance with the dynamics of the game.</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span
        style="
          overflow: hidden;
          display: inline-block;
          margin: 0px 0px;
          border: 0px solid #000000;
          transform: rotate(0rad) translateZ(0px);
          -webkit-transform: rotate(0rad) translateZ(0px);
          width: 624px;
          height: 568px;
        "
        ><img
          alt=""
          src="images/image6.png"
          style="
            width: 630.75px;
            height: 568px;
            margin-left: 0px;
            margin-top: 0px;
            transform: rotate(0rad) translateZ(0px);
            -webkit-transform: rotate(0rad) translateZ(0px);
          "
          title=""
      /></span>
    </p>
    <p class="c2">
      <span class="c0">Figure 2:</span
      ><span class="c4 c3"
        >&nbsp;Atari Pong, K=5. The input, object masks, per-object abstract
        state transitions, and latent space object embeddings are depicted.
        Principal Component Analysis (PCA) was used to map latent
        representations onto the 2D plots.</span
      >
    </p>
    <h3 class="c11" id="h.gs6uidgdvlkf">
      <span class="c14 c3">n-Body Problem</span>
    </h3>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >We further investigate the C-SWM architecture&rsquo;s effectiveness at
        modeling physics scenarios. In [1], the authors report promising results
        for the model&rsquo;s ability to predict the next state of 3-body
        gravitational physics simulations. In contrast to the 2D-block and
        3D-block grid worlds, there are no explicit actions in this environment.
        Instead, the model is provided with two frames of 50x50 pixel images
        that are consecutive in time and thus illustrate implicit action from
        the pair-wise gravitational forces between objects.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c3"
        >Our first contribution is generalizing the 3-body physics simulation
        environment from [9], which requires that all objects have the same
        mass. Our new general simulation environment is able to model </span
      ><span class="c3 c9">n-body</span
      ><span class="c3">&nbsp;gravitational physics between objects of </span
      ><span class="c3 c9">variable mass</span
      ><span class="c4 c3"
        >. This way, we are able to explore more diverse physics environments,
        including those with objects that have distinct properties.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >For our experiments, we first reproduce the results from [1] with the
        3-body environment and objects of equal mass. We also train C-SWM on a
        3-body environment with objects of different masses to investigate if
        and how the model is able to represent systems in which
        &ldquo;actions&rdquo; depend on the properties of the objects (e.g. more
        or less massive objects in our physics simulation will follow different
        trajectories through space and time under the influence of the
        gravitational force from other objects). We hypothesize that such
        scenarios will be more difficult for the model to predict accurately,
        especially over many time steps.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >Additionally, we generate data from a 2-body environment with objects
        of the same mass and a 2-body environment with objects of different
        masses. We maintain training a C-SWM model with 3 object slots on the
        2-body datasets to investigate how the model compensates for more object
        slots than the ground truth number of relevant objects to track. We
        hypothesize that the model will learn some kind of &ldquo;null&rdquo;
        object whose transitions are and position has no effect on the
        representations of &ldquo;real&rdquo; objects.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >Finally, we evaluate the performance of the 3-body, constant mass model
        on our new datasets (3-body with different masses, 2-body with the same
        masses, and 2-body with different masses) to investigate if the model
        has learned general physical laws that generalize to other physical
        environments. Our results are summarized in Table 2.</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span
        style="
          overflow: hidden;
          display: inline-block;
          margin: 0px 0px;
          border: 0px solid #000000;
          transform: rotate(0rad) translateZ(0px);
          -webkit-transform: rotate(0rad) translateZ(0px);
          width: 617.5px;
          height: 414.9px;
        "
        ><img
          alt=""
          src="images/image1.png"
          style="
            width: 628.6px;
            height: 457.33px;
            margin-left: -0px;
            margin-top: -26.9px;
            transform: rotate(0rad) translateZ(0px);
            -webkit-transform: rotate(0rad) translateZ(0px);
          "
          title=""
      /></span>
    </p>
    <p class="c2">
      <span class="c0">Table 2</span><span class="c3">: </span
      ><span class="c0">(a)</span
      ><span class="c3"
        >&nbsp;Ranking results from training the C-SWM model on data from a
        3-body physics simulation with constant mass (mass ratio between objects
        is 1:1:1) and different masses (mass ratio between objects is 1:2:3) and
        from a 2-body physics simulation with constant mass (mass ratio between
        objects is 1:1) and different masses (mass ratio between objects is
        1:2). </span
      ><span class="c0">(b)</span
      ><span class="c4 c3"
        >&nbsp;Ranking results from training the C-SWM model on data from a
        3-body physics simulation with constant mass (mass ratio between objects
        is 1:1:1) and evaluating on each of the other physics scenarios.</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >We found that all models perform near perfect in predicting the correct
        state after 1 step, however as the number of steps increases,
        performance falls off across all models. We see that in the long term,
        the model trained on the 2-body system with constant masses performs the
        best followed by the 3-body system with constant masses. This may be
        attributed to the fact that introducing objects of variable mass creates
        more complicated force relationships between objects for the model to
        learn. In evaluating the baseline 3-body system with constant masses on
        the datasets of the 3-body system with different masses, the 2-body
        system with constant masses, and the 2-body system with different
        masses, we find that it performs best on the 3-body system with
        different masses across all time steps. In all scenarios, but especially
        for the 2-body systems, the model&rsquo;s performance falls off
        drastically as the number of steps increases. This seems to imply that
        the model is not actually learning many generalizable laws of physics as
        much as it is simply identifying patterns within the specific setting it
        was trained on.</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span
        style="
          overflow: hidden;
          display: inline-block;
          margin: 0px 0px;
          border: 0px solid #000000;
          transform: rotate(0rad) translateZ(0px);
          -webkit-transform: rotate(0rad) translateZ(0px);
          width: 624px;
          height: 205.33px;
        "
        ><img
          alt=""
          src="images/image5.png"
          style="
            width: 624px;
            height: 205.33px;
            margin-left: 0px;
            margin-top: 0px;
            transform: rotate(0rad) translateZ(0px);
            -webkit-transform: rotate(0rad) translateZ(0px);
          "
          title=""
      /></span>
    </p>
    <p class="c2">
      <span class="c0">Figure 3</span
      ><span class="c3"
        >: The object masks for each object learned by the models for </span
      ><span class="c0">(a)</span
      ><span class="c3">&nbsp;3-body system with constant mass </span
      ><span class="c0">(b)</span
      ><span class="c3">&nbsp;3-body system with mass ratio 1:2:3 </span
      ><span class="c0">(c)</span
      ><span class="c3">&nbsp;2-body system with constant mass </span
      ><span class="c0">(d)</span
      ><span class="c4 c3">&nbsp;2-body system with mass ratio 1:2</span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c3"
        >Figure 3 shows visualizations of the object extractor learned object
        masks for each of the environments we tested. We see that, across all
        models, each &ldquo;object&rdquo; encoding does not actually correspond
        to any particular object but instead a far less interpretable
        representation of the image as a whole. Thus, when we trained C-SWM
        models with three object slots on data with only two relevant objects,
        we saw no major decrease in performance. Their encodings, as in the
        three object case, are not actually object-centric, so the model can
        easily compensate for these scenarios. This finding also </span
      ><span class="c3 c9">contradicts</span
      ><span class="c4 c3"
        >&nbsp;a major claim in [1]: depending on the downstream task, the C-SWM
        architecture does not necessarily always find the object-centric
        embedding.</span
      >
    </p>
    <h3 class="c11" id="h.t1x99ul887u7">
      <span class="c14 c3">Transfer Learning</span>
    </h3>
    <p class="c2">
      <span class="c4 c3"
        >We investigated the generalization capabilities of our
        Contrastively-trained Structured World Model (C-SWM) across different
        Atari game environments. Our experiments focus on cross-training between
        Space Invaders and Pong to understand the model&#39;s ability to
        transfer learned representations across disparate visual domains. We
        froze the object extractor and object encoder weights and only updated
        the transition model (GNN) weights.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >Our approach involves training the C-SWM on one game environment and
        fine-tuning it on another. This methodology allows us to probe the
        model&#39;s capacity for representation transfer and understand the
        underlying mechanisms of structural scene understanding.</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >While task-specific performance showed limited transfer, we observed
        two critical insights: (1) The model demonstrated rapid convergence of
        latent representations during fine-tuning, suggesting an adaptable
        understanding of scene dynamics. (2) The inability to directly transfer
        performance highlights the complex challenges of cross-domain
        generalization in structured world models.</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >Table 3 presents a tabular summary of the model&#39;s performance,
        measured by the 1-step, 5-step, and 10-step &ldquo;Hits@1 / MRR&quot;
        metrics. This data allows for a quantitative comparison of the
        model&#39;s capabilities across the different training configurations
        and in comparison to the original Space Invaders model.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c0"></span></p>
    <p class="c2">
      <span
        style="
          overflow: hidden;
          display: inline-block;
          margin: 0px 0px;
          border: 0px solid #000000;
          transform: rotate(0rad) translateZ(0px);
          -webkit-transform: rotate(0rad) translateZ(0px);
          width: 624px;
          height: 201.33px;
        "
        ><img
          alt=""
          src="images/image8.png"
          style="
            width: 624px;
            height: 201.33px;
            margin-left: 0px;
            margin-top: 0px;
            transform: rotate(0rad) translateZ(0px);
            -webkit-transform: rotate(0rad) translateZ(0px);
          "
          title=""
      /></span>
    </p>
    <p class="c2">
      <span class="c0">Table 3</span
      ><span class="c4 c3"
        >: Ranking results from training the C-SWM model (k=3) on the Space
        Invaders dataset, comparing the performance of the original model that
        was trained for the task, the original Atari Pong model that has not
        been fine-tuned at all for the task, and the Atari Pong model fine-tuned
        for 50 and 100 epochs.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >Figure 4 provides a visual interpretation of the model&#39;s internal
        representations for each environment and training configuration. The
        model manages to learn a faithful representation of the new game
        environment. While the model may not achieve direct performance parity
        when transferred to a new task, the rapid adaptation of its latent
        representations suggests a possible more generalizable understanding of
        scene composition and relational dynamics.</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c0">Figure 4:</span
      ><span class="c3"
        >&nbsp;Transfer-learned models, K=3. The input, object masks, and latent
        space object embeddings are depicted for the </span
      ><span class="c0">(a)</span
      ><span class="c3"
        >&nbsp;Pong model fine-tuned on the Space Invaders dataset and </span
      ><span class="c0">(b)</span
      ><span class="c3"
        >&nbsp;Space Invaders model fine-tuned on the Pong dataset for 50 and
        100 epochs. </span
      ><span
        style="
          overflow: hidden;
          display: inline-block;
          margin: 0px 0px;
          border: 0px solid #000000;
          transform: rotate(0rad) translateZ(0px);
          -webkit-transform: rotate(0rad) translateZ(0px);
          width: 624px;
          height: 294.67px;
        "
        ><img
          alt=""
          src="images/image4.png"
          style="
            width: 624px;
            height: 294.67px;
            margin-left: 0px;
            margin-top: 0px;
            transform: rotate(0rad) translateZ(0px);
            -webkit-transform: rotate(0rad) translateZ(0px);
          "
          title=""
      /></span>
    </p>
    <h2 class="c6" id="h.h1enr8agkv13">
      <span class="c13 c3">Discussion and Conclusion</span>
    </h2>
    <p class="c2">
      <span class="c4 c3"
        >From our experiments, we understand that C-SWMs perform well when
        applied to scenarios involving visually distinct objects and a fixed
        number of objects, where the model can effectively learn object
        relationships and representations. On the other hand, performance may
        degrade in more complex environments, such as games, where objects can
        vary greatly in appearance or number. This limitation arises from the
        model&rsquo;s reliance on a predefined number of objects, which is fixed
        during training. In terms of the parameter K, which represents the
        number of objects or &quot;nodes&quot; in the GNN, it plays a crucial
        role in the model&#39;s design. As seen in the Pong visualizations, the
        model learns to discard objects that do not exist in a given scene,
        effectively making use of only the relevant slots. Thus, the value of K
        dictates the capacity of the model to handle different numbers of
        objects, and it may be the case that not all slots are necessary for
        certain tasks, which could lead to inefficiencies. Future work could
        explore how to dynamically adjust K or implement more flexible
        representations to better accommodate variable object numbers in more
        complex scenes like games.</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >The n-body experiments demonstrated the limitations of the C-SWM
        architecture in modeling increasingly complex physical systems. We see
        performance drop off significantly as the model tries to predict more
        time steps into the future or additional complications such as variable
        masses are added. Further, we found that the downstream task has a large
        effect on the interpretability of C-SWM embeddings. Although the 3-body
        and 2-body physics simulations involved only a small number of visually
        distinct objects, the encodings learned by the model were no longer
        object-centric.</span
      >
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c4 c3"
        >While the fine-tuned models for Pong and Space Invaders did not perform
        better, more experiments must be done before we can make conclusions on
        the effects of transfer learning with C-SWMs. The GNN component of the
        C-SWM is likely a bottleneck for generalizing just between two different
        games since there is such a drastic change in the latent representations
        of the objects as well as game mechanics. Future work would be to
        compare the models&#39; performances on differing K values (e.g., K=1,
        K=5) and train it for more epochs. In addition, we fine-tuned the Atari
        Pong model on just 100 episodes of the Space Invaders dataset because of
        compute limits, but we could train it on the full 1000 episodes in the
        future.
      </span>
    </p>
    <h2 class="c6" id="h.89jc6sjrd2pp">
      <span class="c13 c3">Future Work</span>
    </h2>
    <p class="c2">
      <span class="c4 c3"
        >Future research on C-SWMs could explore the integration of hard
        negative mining and the InfoNCE loss function to further improve
        contrastive learning. By focusing on more challenging negative examples
        during training by selecting multiple random examples and choosing the
        one that is the most distant from our current state, the model could
        learn to differentiate between similar objects more effectively. We had
        tested the difference in performance between InfoNCE in the Pong (K=3)
        model and saw only improvements when evaluating the next 10 steps and
        decreases in accuracy for 1 and 5 steps. However, we did not train any
        further models, which means this could be a good starting point for
        future work.
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2">
      <span class="c3"
        >Additionally, as noted in previous literature, it was observed that the
        model struggled with discerning </span
      ><span class="c3 c9">visually</span
      ><span class="c4 c3"
        >&nbsp;similar objects, which could lead to confusion propagating to
        downstream tasks. To address this limitation, we propose investigating
        alternative encoder architectures, such as attention-based models or
        more advanced convolutional neural networks, that could better capture
        fine-grained visual features and improve the model&#39;s ability to
        distinguish between subtle differences in object appearance. These
        improvements could enhance the generalization capabilities of C-SWMs and
        make them more robust to a wider range of visual challenges,
        particularly on more complex datasets.</span
      >
    </p>
    <h2 class="c6" id="h.mhk4rj93sjez">
      <span class="c13 c3">Works Cited</span>
    </h2>
    <p class="c2">
      <span class="c4 c3"
        >[1] Kipf, T., van der Pol, E., &amp; Welling, M. (2020). Contrastive
        Learning of Structured World Models. arXiv preprint
        arXiv:1911.12247.</span
      >
    </p>
    <p class="c2">
      <span class="c4 c3"
        >[2] Kipf, T. N., &amp; Welling, M. (2017). Semi-supervised
        classification with graph convolutional networks. International
        Conference on Learning Representations (ICLR).</span
      >
    </p>
    <p class="c2">
      <span class="c4 c3"
        >[3] Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P.,
        &amp; Bengio, Y. (2018). Graph attention networks. International
        Conference on Learning Representations (ICLR).</span
      >
    </p>
    <p class="c2">
      <span class="c4 c3"
        >[4] Xu, K., Hu, W., Leskovec, J., &amp; Jegelka, S. (2019). How
        powerful are graph neural networks? International Conference on Learning
        Representations (ICLR).</span
      >
    </p>
    <p class="c2">
      <span class="c4 c3"
        >[5] Hamilton, W. L., Ying, Z., &amp; Leskovec, J. (2017). Inductive
        representation learning on large graphs. Advances in Neural Information
        Processing Systems (NeurIPS).</span
      >
    </p>
    <p class="c2">
      <span class="c4 c3"
        >[6] Li, Q., Han, Z., &amp; Wu, X.-M. (2018). Deeper insights into graph
        convolutional networks for semi-supervised learning. AAAI Conference on
        Artificial Intelligence.</span
      >
    </p>
    <p class="c2">
      <span class="c4 c3"
        >[7] Ha, D., &amp; Schmidhuber, J. (2018). World models. arXiv preprint
        arXiv:1803.10122.
      </span>
    </p>
    <p class="c2">
      <span class="c3"
        >[8] Collu, C., Caselles-Dupr&eacute;, J., &amp; Houthooft, R. (2020).
        Slot-structured world models. </span
      ><span class="c3 c9">OpenReview.</span>
    </p>
    <p class="c2">
      <span class="c4 c3"
        >[9] Miguel Jaques, Michael Burke, and Timothy Hospedales.
        Physics-as-inverse-graphics: Joint unsupervised learning of objects and
        physics from video. arXiv preprint arXiv:1905.11169, 2019.</span
      >
    </p>
    <p class="c2">
      <span class="c4 c3"
        >&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        &nbsp; &nbsp;
      </span>
    </p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2 c5"><span class="c3 c4"></span></p>
    <p class="c2 c5"><span class="c4 c3"></span></p>
    <p class="c2 c5"><span class="c4 c15"></span></p>
    <p class="c2 c5"><span class="c4 c15"></span></p>
  </body>
</html>
